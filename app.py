import streamlit as st
import pandas as pd
from datetime import datetime
import io
import base64
from news_collector import NewsCollector
from naver_search import NaverNewsSearcher

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ì‹ ë¬¸ ê¸°ì‚¬ ìˆ˜ì§‘ê¸°",
    page_icon="ğŸ“°",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS ìŠ¤íƒ€ì¼ë§
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .category-header {
        font-size: 1.2rem;
        font-weight: bold;
        color: #333;
        margin: 1rem 0 0.5rem 0;
        background-color: #f0f2f6;
        padding: 0.5rem;
        border-radius: 5px;
    }
    .article-item {
        background-color: #f8f9fa;
        padding: 1rem;
        margin: 0.5rem 0;
        border-radius: 5px;
        border-left: 4px solid #1f77b4;
    }
    .search-box {
        background-color: #e9ecef;
        padding: 1rem;
        border-radius: 5px;
        margin: 1rem 0;
    }
    .newspaper-section {
        border: 1px solid #ddd;
        border-radius: 5px;
        padding: 1rem;
        margin: 0.5rem 0;
    }
    .stButton > button {
        width: 100%;
    }
</style>
""", unsafe_allow_html=True)

def check_secrets():
    """secrets ì„¤ì • í™•ì¸ - í™”ë©´ì— ë…¸ì¶œí•˜ì§€ ì•ŠìŒ"""
    missing_secrets = []
    api_available = False
    
    try:
        client_id = st.secrets["naver_api"]["client_id"]
        client_secret = st.secrets["naver_api"]["client_secret"]
        if client_id and client_secret:
            api_available = True
    except KeyError:
        missing_secrets.append("ë„¤ì´ë²„ API í‚¤")
    
    return api_available, missing_secrets

def main():
    # secrets í™•ì¸ (ë³´ì•ˆ)
    api_available, missing_secrets = check_secrets()
    
    if missing_secrets:
        st.sidebar.warning(f"âš ï¸ ì„¤ì •ë˜ì§€ ì•Šì€ í•­ëª©: {', '.join(missing_secrets)}")
        st.sidebar.info("ì¼ë¶€ ê¸°ëŠ¥ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    st.markdown('<h1 class="main-header">ğŸ“° ì‹ ë¬¸ ê¸°ì‚¬ ìˆ˜ì§‘ê¸°</h1>', unsafe_allow_html=True)
    
    # ì‚¬ì´ë“œë°”ì— ì„¤ì • ì •ë³´ í‘œì‹œ (ë³´ì•ˆ ì •ë³´ ìˆ¨ê¹€)
    with st.sidebar:
        st.markdown("### âš™ï¸ ì„¤ì • ì •ë³´")
        
        # API í‚¤ ê°’ì„ ë…¸ì¶œí•˜ì§€ ì•Šê³  ìƒíƒœë§Œ í‘œì‹œ
        api_status = "âœ… ì„¤ì •ë¨" if api_available else "âŒ ë¯¸ì„¤ì •"
        st.info(f"ë„¤ì´ë²„ API: {api_status}")
        
        # ê¸°íƒ€ ì„¤ì • ì •ë³´ (ë¯¼ê°í•˜ì§€ ì•Šì€ ì •ë³´ë§Œ)
        try:
            max_articles = st.secrets["app_settings"]["max_articles_per_request"]
            st.info(f"ìµœëŒ€ ìš”ì²­ ê¸°ì‚¬ ìˆ˜: {max_articles}")
        except:
            st.info("ê¸°ë³¸ ì„¤ì • ì‚¬ìš© ì¤‘")
        
        st.markdown("---")
        st.markdown("### ğŸ“– ì‚¬ìš©ë²•")
        st.markdown("""
        **ì‹ ë¬¸ ê²Œì¬ ê¸°ì‚¬ ìˆ˜ì§‘:**
        1. ì›í•˜ëŠ” ì‹ ë¬¸ì‚¬ ì„ íƒ
        2. ë‚ ì§œ ì„ íƒ
        3. í¬ë¡¤ë§ ì‹œì‘
        
        **ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰:**
        1. í‚¤ì›Œë“œ ì…ë ¥
        2. ìµœëŒ€ ê¸°ì‚¬ ìˆ˜ ì„ íƒ
        3. ê²€ìƒ‰ ì‹œì‘
        """)
        
        st.markdown("---")
        st.markdown("### ğŸ“Š í†µê³„")
        if 'newspaper_articles' in st.session_state:
            st.metric("ìˆ˜ì§‘ëœ ì‹ ë¬¸ ê¸°ì‚¬", len(st.session_state['newspaper_articles']))
        if 'search_articles' in st.session_state:
            st.metric("ê²€ìƒ‰ëœ ê¸°ì‚¬", len(st.session_state['search_articles']))
    
    # íƒ­ ìƒì„±
    tab1, tab2 = st.tabs(["ğŸ“° ì‹ ë¬¸ ê²Œì¬ ê¸°ì‚¬ ìˆ˜ì§‘", "ğŸ” ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰"])
    
    with tab1:
        newspaper_collection_tab()
    
    with tab2:
        naver_search_tab()

def newspaper_collection_tab():
    st.markdown("### ì‹ ë¬¸ ê²Œì¬ ê¸°ì‚¬ ìˆ˜ì§‘")
    st.markdown("ì‹ ë¬¸ì— ê²Œì¬ëœ ê¸°ì‚¬ë§Œ ìˆ˜ì§‘í•˜ì—¬ ì œê³µí•©ë‹ˆë‹¤.")
    
    # ë‚ ì§œ ì„ íƒì„ ë§¨ ìœ„ë¡œ ì´ë™
    selected_date = st.date_input(
        "ğŸ“… ìˆ˜ì§‘í•  ë‚ ì§œ ì„ íƒ",
        value=datetime.now().date(),
        max_value=datetime.now().date(),
        help="ìˆ˜ì§‘í•˜ê³  ì‹¶ì€ ì‹ ë¬¸ ë°œí–‰ì¼ì„ ì„ íƒí•˜ì„¸ìš”",
        key="date_picker"
    )
    
    st.markdown("---")
    
    # ì‹ ë¬¸ì‚¬ ì„ íƒ ì„¹ì…˜
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.markdown('<div class="category-header">ğŸ“Š ê²½ì œ ì‹ ë¬¸</div>', unsafe_allow_html=True)
        with st.container():
            economic_papers = {
                "ë§¤ì¼ê²½ì œ": "009",
                "ë¨¸ë‹ˆíˆ¬ë°ì´": "008", 
                "ì„œìš¸ê²½ì œ": "011",
                "ì´ë°ì¼ë¦¬": "018",
                "íŒŒì´ë‚¸ì…œë‰´ìŠ¤": "014",
                "í•œêµ­ê²½ì œ": "015"
            }
            
            economic_all = st.checkbox("ì „ì²´ ì„ íƒ", key="economic_all")
            economic_selected = []
            
            for paper, oid in economic_papers.items():
                checked = st.checkbox(paper, value=economic_all, key=f"economic_{oid}")
                if checked:
                    economic_selected.append((paper, oid))
    
    with col2:
        st.markdown('<div class="category-header">ğŸ“‹ ì¢…í•©ì¼ê°„ì§€(ì¡°ê°„)</div>', unsafe_allow_html=True)
        with st.container():
            general_papers = {
                "ê²½í–¥ì‹ ë¬¸": "032",
                "êµ­ë¯¼ì¼ë³´": "005",
                "ë™ì•„ì¼ë³´": "020",
                "ì„œìš¸ì‹ ë¬¸": "081",
                "ì„¸ê³„ì¼ë³´": "022",
                "ì¡°ì„ ì¼ë³´": "023",
                "ì¤‘ì•™ì¼ë³´": "025",
                "í•œê²¨ë ˆ": "028",
                "í•œêµ­ì¼ë³´": "469",
                "ë””ì§€í„¸íƒ€ì„ìŠ¤": "029",
                "ì „ìì‹ ë¬¸": "030"
            }
            
            general_all = st.checkbox("ì „ì²´ ì„ íƒ", key="general_all")
            general_selected = []
            
            for paper, oid in general_papers.items():
                checked = st.checkbox(paper, value=general_all, key=f"general_{oid}")
                if checked:
                    general_selected.append((paper, oid))
    
    with col3:
        st.markdown('<div class="category-header">ğŸŒ† ì„ê°„ ì‹ ë¬¸</div>', unsafe_allow_html=True)
        with st.container():
            evening_papers = {
                "ë¬¸í™”ì¼ë³´": "021",
                "í—¤ëŸ´ë“œê²½ì œ": "016", 
                "ì•„ì‹œì•„ê²½ì œ": "277"
            }
            
            evening_all = st.checkbox("ì „ì²´ ì„ íƒ", key="evening_all")
            evening_selected = []
            
            for paper, oid in evening_papers.items():
                checked = st.checkbox(paper, value=evening_all, key=f"evening_{oid}")
                if checked:
                    evening_selected.append((paper, oid))
    
    st.markdown("---")
    
    # í¬ë¡¤ë§ ì‹œì‘ ë²„íŠ¼
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        if st.button("ğŸš€ í¬ë¡¤ë§ ì‹œì‘", type="primary", use_container_width=True, key="btn_start_crawling"):
            all_selected = economic_selected + general_selected + evening_selected
            
            if not all_selected:
                st.error("âŒ ìµœì†Œ í•˜ë‚˜ì˜ ì‹ ë¬¸ì‚¬ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
                return
            
            # ì§„í–‰ ìƒí™© í‘œì‹œ
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            collector = NewsCollector()
            
            try:
                status_text.text(f"ğŸš€ {len(all_selected)}ê°œ ì‹ ë¬¸ì‚¬ ë³‘ë ¬ ìˆ˜ì§‘ ì‹œì‘...")
                progress_bar.progress(20)
                
                # ë³‘ë ¬ ì²˜ë¦¬ë¡œ ë¹ ë¥´ê²Œ ìˆ˜ì§‘ (í…Œì´ë¸” í˜•íƒœë¡œ ìƒíƒœ í‘œì‹œ)
                all_articles = collector.crawl_multiple_papers(all_selected, selected_date.strftime("%Y%m%d"))
                
                progress_bar.progress(80)
                
                # ì¤‘ë³µ ì œê±°
                unique_articles = remove_duplicates(all_articles)
                
                # ì„¸ì…˜ ìƒíƒœì— ì €ì¥
                st.session_state['newspaper_articles'] = unique_articles
                st.session_state['paper_date'] = selected_date
                
                status_text.text(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ! ì´ {len(unique_articles)}ê°œ ê¸°ì‚¬")
                progress_bar.progress(100)
                
                if len(unique_articles) == 0:
                    st.warning("âš ï¸ ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ë‚ ì§œë‚˜ ì‹ ë¬¸ì‚¬ë¥¼ ì„ íƒí•´ë³´ì„¸ìš”.")
                else:
                    st.success(f"ğŸ‰ {len(all_selected)}ê°œ ì‹ ë¬¸ì‚¬ì—ì„œ ì´ {len(unique_articles)}ê°œì˜ ê¸°ì‚¬ë¥¼ ì„±ê³µì ìœ¼ë¡œ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤!")
                    
            except Exception as e:
                st.error(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            finally:
                collector.close()
    
    # ê²°ê³¼ í‘œì‹œ
    if 'newspaper_articles' in st.session_state:
        display_newspaper_results()

def display_newspaper_results():
    articles = st.session_state['newspaper_articles']
    
    st.markdown("---")
    
    # ê²°ê³¼ í‘œì‹œ (ê²€ìƒ‰ ê¸°ëŠ¥ì„ ì•„ë˜ë¡œ ì´ë™)
    st.markdown(f"### ğŸ“° {st.session_state['paper_date'].strftime('%Yë…„ %mì›” %dì¼')}ì˜ ì‹ ë¬¸ ê²Œì¬ ê¸°ì‚¬ ëª¨ìŒ")
    st.markdown(f"**ì´ {len(articles)}ê°œ ê¸°ì‚¬**")
    
    if len(articles) == 0:
        st.info("ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ë‹¤ìš´ë¡œë“œ ê¸°ëŠ¥ì„ ìƒë‹¨ìœ¼ë¡œ (ê¸°ì¡´ ìœ„ì¹˜ ìœ ì§€)
    st.markdown("### ğŸ’¾ ë‹¤ìš´ë¡œë“œ")
    col1, col2, col3 = st.columns(3)
    
    with col1:
        excel_data = create_excel_download(articles)
        st.download_button(
            label="ğŸ“Š ì—‘ì…€ ë‹¤ìš´ë¡œë“œ",
            data=excel_data,
            file_name=f"newspaper_articles_{st.session_state['paper_date'].strftime('%Y%m%d')}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            key="btn_download_newspaper_excel"
        )
    
    with col2:
        text_data = create_text_download(articles, st.session_state['paper_date'])
        st.download_button(
            label="ğŸ“„ í…ìŠ¤íŠ¸ ë‹¤ìš´ë¡œë“œ",
            data=text_data,
            file_name=f"newspaper_articles_{st.session_state['paper_date'].strftime('%Y%m%d')}.txt",
            mime="text/plain",
            key="btn_download_newspaper_text"
        )
    
    with col3:
        if st.button("ğŸ“‹ í´ë¦½ë³´ë“œ ë³µì‚¬", key="btn_copy_newspaper_text"):
            copy_text = create_text_download(articles, st.session_state['paper_date'])
            st.code(copy_text, language="text")
            st.success("âœ… í…ìŠ¤íŠ¸ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤. ìœ„ ë‚´ìš©ì„ ë³µì‚¬í•˜ì„¸ìš”.")
    
    st.markdown("---")
    
    # ê²€ìƒ‰ ê¸°ëŠ¥ì„ ë‹¤ìš´ë¡œë“œ ì•„ë˜ë¡œ ì´ë™
    st.markdown('<div class="search-box">', unsafe_allow_html=True)
    col1, col2, col3 = st.columns([3, 1, 1])
    
    with col1:
        search_term = st.text_input("ğŸ” ê¸°ì‚¬ ê²€ìƒ‰", placeholder="ì œëª©ìœ¼ë¡œ ê²€ìƒ‰...", key="input_search_articles_newspaper")
    
    with col2:
        if st.button("ê²€ìƒ‰", key="btn_search_articles_newspaper"):
            if search_term:
                filtered_articles = [
                    article for article in articles 
                    if search_term.lower() in article['title'].lower()
                ]
                st.session_state['filtered_articles'] = filtered_articles
            else:
                st.session_state['filtered_articles'] = articles
    
    with col3:
        if st.button("ì´ˆê¸°í™”", key="btn_reset_articles_newspaper"):
            st.session_state['filtered_articles'] = articles
            st.rerun()
    
    st.markdown('</div>', unsafe_allow_html=True)
    
    # í‘œì‹œí•  ê¸°ì‚¬ ê²°ì •
    display_articles = st.session_state.get('filtered_articles', articles)
    
    st.markdown("---")
    
    # ì‹ ë¬¸ì‚¬ë³„ë¡œ ê·¸ë£¹í™”
    newspaper_groups = {}
    for article in display_articles:
        newspaper = article['newspaper']
        if newspaper not in newspaper_groups:
            newspaper_groups[newspaper] = []
        newspaper_groups[newspaper].append(article)
    
    # ì‹ ë¬¸ì‚¬ë³„ ê¸°ì‚¬ í‘œì‹œ
    for newspaper, articles_list in newspaper_groups.items():
        st.markdown(f"#### ğŸ“Œ [{newspaper}] ({len(articles_list)}ê°œ)")
        for article in articles_list:
            page_info = f"[{article['page']}] " if article['page'] else ""
            st.markdown(f"ğŸ”¹ {page_info}[{article['title']}]({article['url']})")

def naver_search_tab():
    st.markdown("### ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰")
    st.markdown("ë„¤ì´ë²„ ê²€ìƒ‰ APIë¥¼ ì´ìš©í•˜ì—¬ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        keyword = st.text_input("ğŸ” ê²€ìƒ‰ í‚¤ì›Œë“œ", placeholder="ê²€ìƒ‰í•  í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”...", key="input_search_keyword")
    
    with col2:
        # selectboxë¥¼ number_inputìœ¼ë¡œ ë³€ê²½
        max_articles = st.number_input(
            "ìµœëŒ€ ê¸°ì‚¬ ìˆ˜", 
            min_value=1, 
            max_value=1000, 
            value=100, 
            step=1, 
            key="input_max_articles",
            help="1ë¶€í„° 1000ê¹Œì§€ ì…ë ¥ ê°€ëŠ¥í•©ë‹ˆë‹¤"
        )
    
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        if st.button("ğŸ” ê²€ìƒ‰ ì‹œì‘", type="primary", use_container_width=True, key="btn_start_search"):
            if not keyword:
                st.error("âŒ ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                return
            
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            searcher = NaverNewsSearcher()
            
            try:
                status_text.text("ğŸ” ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘...")
                progress_bar.progress(50)
                
                articles = searcher.search_news(keyword, max_articles)
                
                progress_bar.progress(100)
                status_text.text(f"âœ… ê²€ìƒ‰ ì™„ë£Œ! ì´ {len(articles)}ê°œ ê¸°ì‚¬")
                
                # ì„¸ì…˜ ìƒíƒœì— ì €ì¥
                st.session_state['search_articles'] = articles
                st.session_state['current_search_keyword'] = keyword
                
                if len(articles) == 0:
                    st.warning("âš ï¸ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ì‹œë„í•´ë³´ì„¸ìš”.")
                else:
                    st.success(f"ğŸ‰ '{keyword}'ì— ëŒ€í•œ {len(articles)}ê°œì˜ ê¸°ì‚¬ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤!")
                
            except Exception as e:
                st.error(f"âŒ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
                progress_bar.progress(0)
                status_text.text("")
    
    # ê²€ìƒ‰ ê²°ê³¼ í‘œì‹œ
    if 'search_articles' in st.session_state:
        display_search_results()

def display_search_results():
    articles = st.session_state['search_articles']
    keyword = st.session_state['current_search_keyword']
    
    st.markdown("---")
    st.markdown(f"### ğŸ” '{keyword}' ê²€ìƒ‰ ê²°ê³¼ ({len(articles)}ê°œ)")
    
    if len(articles) == 0:
        st.info("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ë‹¤ìš´ë¡œë“œ ê¸°ëŠ¥ì„ ìƒë‹¨ìœ¼ë¡œ ì´ë™
    st.markdown("### ğŸ’¾ ë‹¤ìš´ë¡œë“œ")
    col1, col2, col3 = st.columns(3)
    
    with col1:
        excel_data = create_search_excel_download(articles)
        st.download_button(
            label="ğŸ“Š ì—‘ì…€ ë‹¤ìš´ë¡œë“œ",
            data=excel_data,
            file_name=f"search_results_{keyword}_{datetime.now().strftime('%Y%m%d')}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            key="btn_download_naver_search_excel"
        )
    
    with col2:
        csv_data = create_search_csv_download(articles)
        st.download_button(
            label="ğŸ“Š CSV ë‹¤ìš´ë¡œë“œ",
            data=csv_data,
            file_name=f"search_results_{keyword}_{datetime.now().strftime('%Y%m%d')}.csv",
            mime="text/csv",
            key="btn_download_naver_search_csv"
        )
    
    with col3:
        text_data = create_search_text_download(articles, keyword)
        st.download_button(
            label="ğŸ“„ í…ìŠ¤íŠ¸ ë‹¤ìš´ë¡œë“œ",
            data=text_data,
            file_name=f"search_results_{keyword}_{datetime.now().strftime('%Y%m%d')}.txt",
            mime="text/plain",
            key="btn_download_naver_search_text"
        )
    
    st.markdown("---")
    
    # ê²°ê³¼ í‘œì‹œ ì˜µì…˜
    display_mode = st.radio("í‘œì‹œ ë°©ì‹", ["ìš”ì•½ ë³´ê¸°", "ì „ì²´ ë³´ê¸°"], horizontal=True, key="radio_display_mode")
    
    if display_mode == "ìš”ì•½ ë³´ê¸°":
        # ê°„ë‹¨í•œ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ í‘œì‹œ
        for i, article in enumerate(articles, 1):
            st.markdown(f"**{i}.** [{article['title']}]({article['link']})")
            st.caption(f"ğŸ“… {article['pubDate']} | ğŸ“° {article.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')}")
            if article.get('description'):
                st.write(f"ğŸ’¬ {article['description'][:100]}...")
            st.markdown("---")
    else:
        # ìƒì„¸í•œ expander í˜•íƒœë¡œ í‘œì‹œ
        for i, article in enumerate(articles, 1):
            with st.expander(f"{i}. {article['title']}", expanded=False):
                st.markdown(f"**ìš”ì•½:** {article['description']}")
                st.markdown(f"**ë°œí–‰ì¼:** {article['pubDate']}")
                st.markdown(f"**ì¶œì²˜:** {article.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')}")
                st.markdown(f"**ë§í¬:** [ê¸°ì‚¬ ë³´ê¸°]({article['link']})")

# í—¬í¼ í•¨ìˆ˜ë“¤
def remove_duplicates(articles):
    """ì¤‘ë³µ ê¸°ì‚¬ ì œê±°"""
    seen_urls = set()
    unique_articles = []
    
    for article in articles:
        if article['url'] not in seen_urls:
            seen_urls.add(article['url'])
            unique_articles.append(article)
    
    return unique_articles

def create_excel_download(articles):
    """ì—‘ì…€ íŒŒì¼ ìƒì„±"""
    df = pd.DataFrame(articles)
    output = io.BytesIO()
    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        df.to_excel(writer, index=False, sheet_name='ì‹ ë¬¸ê¸°ì‚¬')
    return output.getvalue()

def create_text_download(articles, date):
    """í…ìŠ¤íŠ¸ íŒŒì¼ ìƒì„±"""
    text_content = f"ğŸ“° {date.strftime('%Yë…„ %mì›” %dì¼')}ì˜ ì‹ ë¬¸ ê²Œì¬ ê¸°ì‚¬ ëª¨ìŒ\n\n"
    
    newspaper_groups = {}
    for article in articles:
        newspaper = article['newspaper']
        if newspaper not in newspaper_groups:
            newspaper_groups[newspaper] = []
        newspaper_groups[newspaper].append(article)
    
    for newspaper, articles_list in newspaper_groups.items():
        text_content += f"ğŸ“Œ [{newspaper}]\n"
        for article in articles_list:
            page_info = f"[{article['page']}] " if article['page'] else ""
            text_content += f"ğŸ”¹ {page_info}{article['title']}\n   {article['url']}\n"
        text_content += "\n"
    
    return text_content

def create_search_excel_download(articles):
    """ê²€ìƒ‰ ê²°ê³¼ ì—‘ì…€ íŒŒì¼ ìƒì„±"""
    df = pd.DataFrame(articles)
    output = io.BytesIO()
    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        df.to_excel(writer, index=False, sheet_name='ê²€ìƒ‰ê²°ê³¼')
    return output.getvalue()

def create_search_csv_download(articles):
    """ê²€ìƒ‰ ê²°ê³¼ CSV íŒŒì¼ ìƒì„±"""
    df = pd.DataFrame(articles)
    return df.to_csv(index=False).encode('utf-8-sig')

def create_search_text_download(articles, keyword):
    """ê²€ìƒ‰ ê²°ê³¼ í…ìŠ¤íŠ¸ íŒŒì¼ ìƒì„±"""
    text_content = f"ğŸ” '{keyword}' ê²€ìƒ‰ ê²°ê³¼\n"
    text_content += f"ê²€ìƒ‰ì¼ì‹œ: {datetime.now().strftime('%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„')}\n"
    text_content += f"ì´ {len(articles)}ê°œ ê¸°ì‚¬\n\n"
    
    for i, article in enumerate(articles, 1):
        text_content += f"{i}. {article['title']}\n"
        text_content += f"   ìš”ì•½: {article['description']}\n"
        text_content += f"   ë°œí–‰ì¼: {article['pubDate']}\n"
        text_content += f"   ì¶œì²˜: {article.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')}\n"
        text_content += f"   ë§í¬: {article['link']}\n\n"
    
    return text_content

if __name__ == "__main__":
    main()