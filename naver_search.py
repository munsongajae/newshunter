import requests
import json
from datetime import datetime, timedelta
import time
import streamlit as st
import re
from urllib.parse import quote
from bs4 import BeautifulSoup
import pandas as pd

class NaverNewsSearcher:
    def __init__(self):
        # Streamlit secretsì—ì„œ API í‚¤ ë¡œë“œ
        try:
            self.client_id = st.secrets["naver_api"]["client_id"]
            self.client_secret = st.secrets["naver_api"]["client_secret"]
            self.api_available = True
        except KeyError:
            st.warning("âš ï¸ ë„¤ì´ë²„ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì›¹ í¬ë¡¤ë§ ë°©ì‹ìœ¼ë¡œ ë™ì‘í•©ë‹ˆë‹¤.")
            self.client_id = None
            self.client_secret = None
            self.api_available = False
        
        self.base_url = "https://openapi.naver.com/v1/search/news.json"
        
        # ì„¤ì •ê°’ ë¡œë“œ
        try:
            self.max_articles_per_request = st.secrets["app_settings"]["max_articles_per_request"]
            self.request_delay = st.secrets["app_settings"]["request_delay"]
        except KeyError:
            self.max_articles_per_request = 100
            self.request_delay = 0.1

        # ì„¸ì…˜ ì„¤ì •
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
        })

    def search_news(self, keyword, max_results=100):
        """ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ - API ìš°ì„ , ì‹¤íŒ¨ì‹œ í¬ë¡¤ë§"""
        if self.api_available:
            try:
                return self.search_news_api(keyword, max_results)
            except Exception as e:
                st.warning(f"API ê²€ìƒ‰ ì‹¤íŒ¨: {e}. ì›¹ í¬ë¡¤ë§ìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                return self.search_news_fallback(keyword, max_results)
        else:
            return self.search_news_fallback(keyword, max_results)

    def search_news_api(self, keyword, max_results=100):
        """ë„¤ì´ë²„ ë‰´ìŠ¤ APIë¥¼ ì‚¬ìš©í•œ ê²€ìƒ‰"""
        articles = []
        display = min(self.max_articles_per_request, 100)
        
        total_requests = (max_results + display - 1) // display
        
        for i in range(total_requests):
            start = i * display + 1
            current_display = min(display, max_results - len(articles))
            
            if current_display <= 0:
                break
            
            params = {
                'query': keyword,
                'display': current_display,
                'start': start,
                'sort': 'date'
            }
            
            headers = {
                'X-Naver-Client-Id': self.client_id,
                'X-Naver-Client-Secret': self.client_secret,
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(self.base_url, params=params, headers=headers)
            
            if response.status_code == 200:
                data = response.json()
                items = data.get('items', [])
                
                for item in items:
                    article = {
                        'title': self.clean_html_tags(item['title']),
                        'link': item['link'],
                        'description': self.clean_html_tags(item['description']),
                        'pubDate': self.format_date(item['pubDate']),
                        'source': self.extract_source(item.get('link', ''))
                    }
                    articles.append(article)
                
                if len(items) < current_display:
                    break
                    
            else:
                raise Exception(f"API ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
            
            time.sleep(self.request_delay)
        
        return articles[:max_results]

    def search_news_fallback(self, keyword, max_results=100):
        """API í‚¤ê°€ ì—†ëŠ” ê²½ìš° ì›¹ í¬ë¡¤ë§ìœ¼ë¡œ ëŒ€ì²´"""
        articles = []
        
        try:
            page_size = 10
            total_pages = (max_results + page_size - 1) // page_size
            
            for page_num in range(1, total_pages + 1):
                if len(articles) >= max_results:
                    break
                
                start = (page_num - 1) * page_size + 1
                search_url = f"https://search.naver.com/search.naver?where=news&query={quote(keyword)}&start={start}"
                
                try:
                    response = self.session.get(search_url, timeout=30)
                    response.raise_for_status()
                    
                    soup = BeautifulSoup(response.content, 'html.parser')
                    news_items = soup.find_all('div', class_='news_area')
                    
                    if not news_items:
                        break
                    
                    for item in news_items:
                        if len(articles) >= max_results:
                            break
                        
                        try:
                            title_element = item.find('a', class_='news_tit')
                            if not title_element:
                                continue
                            
                            title = title_element.get_text().strip()
                            link = title_element.get('href')
                            
                            desc_element = item.find('div', class_='news_dsc')
                            description = desc_element.get_text().strip() if desc_element else ''
                            
                            info_element = item.find('span', class_='info')
                            source = ''
                            pub_date = ''
                            
                            if info_element:
                                info_text = info_element.get_text()
                                parts = info_text.split('Â·')
                                if len(parts) >= 2:
                                    source = parts[0].strip()
                                    pub_date = parts[-1].strip()
                            
                            articles.append({
                                'title': title,
                                'link': link,
                                'description': description,
                                'pubDate': pub_date,
                                'source': source
                            })
                            
                        except Exception as e:
                            continue
                    
                    time.sleep(1)
                    
                except Exception as e:
                    st.warning(f"í˜ì´ì§€ {page_num} í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
                    continue
                
        except Exception as e:
            st.error(f"ì›¹ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return articles[:max_results]

    def clean_html_tags(self, text):
        """HTML íƒœê·¸ ì œê±°"""
        if not text:
            return ""
        clean = re.compile('<.*?>')
        return re.sub(clean, '', text)

    def format_date(self, date_str):
        """ë‚ ì§œ í¬ë§· ë³€í™˜"""
        try:
            from email.utils import parsedate_to_datetime
            dt = parsedate_to_datetime(date_str)
            return dt.strftime('%Y.%m.%d %H:%M')
        except:
            return date_str

    def extract_source(self, url):
        """URLì—ì„œ ì¶œì²˜ ì¶”ì¶œ"""
        try:
            if 'news.naver.com' in url:
                return 'ë„¤ì´ë²„ë‰´ìŠ¤'
            elif 'chosun.com' in url:
                return 'ì¡°ì„ ì¼ë³´'
            elif 'joongang.co.kr' in url:
                return 'ì¤‘ì•™ì¼ë³´'
            elif 'donga.com' in url:
                return 'ë™ì•„ì¼ë³´'
            elif 'hankyung.com' in url:
                return 'í•œêµ­ê²½ì œ'
            elif 'mk.co.kr' in url:
                return 'ë§¤ì¼ê²½ì œ'
            elif 'hani.co.kr' in url:
                return 'í•œê²¨ë ˆ'
            elif 'khan.co.kr' in url:
                return 'ê²½í–¥ì‹ ë¬¸'
            else:
                return 'ê¸°íƒ€'
        except:
            return 'ì•Œ ìˆ˜ ì—†ìŒ'

    def search_stock_news(self, keywords, selected_date, max_articles):
        """íŠ¹ì§•ì£¼ ê´€ë ¨ ë‰´ìŠ¤ ê²€ìƒ‰"""
        try:
            all_results = []
            display = min(self.max_articles_per_request, 100)  # API í•œ ë²ˆ í˜¸ì¶œë‹¹ ìµœëŒ€ 100ê°œ
            
            # ê° í‚¤ì›Œë“œë³„ë¡œ ë™ì¼í•œ ìˆ˜ì˜ ê¸°ì‚¬ ê²€ìƒ‰
            articles_per_keyword = max_articles // len(keywords)
            
            # ê° í‚¤ì›Œë“œë³„ë¡œ ê²€ìƒ‰
            for keyword in keywords:
                keyword_results = []
                total_requests = (articles_per_keyword + display - 1) // display
                
                for i in range(total_requests):
                    start = i * display + 1
                    current_display = min(display, articles_per_keyword - len(keyword_results))
                    
                    if current_display <= 0:
                        break
                    
                    # API í˜¸ì¶œ íŒŒë¼ë¯¸í„° ì„¤ì •
                    params = {
                        "query": keyword,
                        "display": current_display,
                        "start": start,
                        "sort": "date"   # ìµœì‹ ìˆœ ì •ë ¬
                    }
                    
                    # API í˜¸ì¶œ
                    headers = {
                        'X-Naver-Client-Id': self.client_id,
                        'X-Naver-Client-Secret': self.client_secret
                    }
                    
                    response = requests.get(self.base_url, headers=headers, params=params)
                    response.raise_for_status()
                    
                    # ì‘ë‹µ íŒŒì‹±
                    data = response.json()
                    items = data.get("items", [])
                    
                    # ê¸°ì‚¬ ì •ë³´ ì¶”ì¶œ
                    for item in items:
                        # HTML íƒœê·¸ ì œê±°
                        title = re.sub(r'<[^>]+>', '', item.get("title", ""))
                        description = re.sub(r'<[^>]+>', '', item.get("description", ""))
                        
                        # ë°œí–‰ì¼ íŒŒì‹±
                        pub_date = item.get("pubDate", "")
                        try:
                            pub_date = datetime.strptime(pub_date, "%a, %d %b %Y %H:%M:%S +0900")
                            
                            # ì„ íƒëœ ë‚ ì§œì™€ ì¼ì¹˜í•˜ëŠ” ê¸°ì‚¬ë§Œ í•„í„°ë§
                            if pub_date.date() != selected_date:
                                continue
                            
                        except ValueError:
                            continue
                        
                        keyword_results.append({
                            "title": title,
                            "description": description,
                            "link": item.get("link", ""),
                            "pubDate": pub_date,
                            "keyword": keyword  # í‚¤ì›Œë“œ ì •ë³´ ì¶”ê°€
                        })
                    
                    if len(items) < current_display:
                        break
                    
                    time.sleep(self.request_delay)  # API í˜¸ì¶œ ê°„ê²© ì¡°ì ˆ
                
                # ê° í‚¤ì›Œë“œë³„ ê²°ê³¼ë¥¼ ì „ì²´ ê²°ê³¼ì— ì¶”ê°€
                all_results.extend(keyword_results[:articles_per_keyword])
            
            # ì¤‘ë³µ ì œê±° (ì œëª© ê¸°ì¤€)
            seen_titles = set()
            unique_results = []
            for result in all_results:
                if result['title'] not in seen_titles:
                    seen_titles.add(result['title'])
                    unique_results.append(result)
            
            # ë°œí–‰ì¼ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
            unique_results.sort(key=lambda x: x['pubDate'], reverse=True)
            
            return unique_results[:max_articles]  # ìš”ì²­í•œ ìµœëŒ€ ê°œìˆ˜ë§Œí¼ ë°˜í™˜
            
        except Exception as e:
            st.error(f"ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return []

def display_stock_news_tab():
    """íŠ¹ì§•ì£¼ í¬ì°© íƒ­ í‘œì‹œ"""
    st.markdown("### ğŸ“ˆ íŠ¹ì§•ì£¼ í¬ì°©")
    
    # ê²€ìƒ‰ í‚¤ì›Œë“œ ì„ íƒ
    default_keywords = ["íŠ¹ì§•ì£¼", "ê¸‰ë“±ì£¼", "ìƒí•œê°€", "í•˜í•œê°€", "ê¸‰ë“±ì„¸", "ê¸‰ë½ì„¸", 
                       "ê°•ì„¸", "ì•½ì„¸", "ê±°ë˜ëŸ‰ ì¦ê°€", "ì‹ ê³ ê°€", "ì‹ ì €ê°€"]
    
    # ì‚¬ìš©ì ì •ì˜ í‚¤ì›Œë“œ ì…ë ¥
    custom_keyword = st.text_input(
        "ì¶”ê°€ ê²€ìƒ‰ í‚¤ì›Œë“œ ì…ë ¥",
        help="ì›í•˜ëŠ” ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”. ì…ë ¥ í›„ Enterë¥¼ ëˆ„ë¥´ë©´ í‚¤ì›Œë“œê°€ ì¶”ê°€ë©ë‹ˆë‹¤."
    )
    
    # ì‚¬ìš©ì ì •ì˜ í‚¤ì›Œë“œê°€ ì…ë ¥ë˜ë©´ default_keywordsì— ì¶”ê°€
    if custom_keyword and custom_keyword not in default_keywords:
        default_keywords.append(custom_keyword)
    
    # í‚¤ì›Œë“œ ì„ íƒ (ê¸°ë³¸ í‚¤ì›Œë“œ + ì‚¬ìš©ì ì •ì˜ í‚¤ì›Œë“œ)
    selected_keywords = st.multiselect(
        "ê²€ìƒ‰ í‚¤ì›Œë“œ ì„ íƒ(10ê°œ ì´í•˜)",
        options=default_keywords,
        default=default_keywords[:3]
    )
    
    # ì¡°íšŒ ë‚ ì§œ ì„ íƒ (ì‹œì¥ ë°ì´í„°ìš©)
    today = datetime.now()
    # ì£¼ë§ì¸ ê²½ìš° ê¸ˆìš”ì¼ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ
    if today.weekday() >= 5:  # 5: í† ìš”ì¼, 6: ì¼ìš”ì¼
        days_to_subtract = today.weekday() - 4
        today = today - timedelta(days=days_to_subtract)
    
    selected_date = st.date_input(
        "ì‹œì¥ ë°ì´í„° ì¡°íšŒ ë‚ ì§œ",
        value=today,
        max_value=today,
        help="ì‹œì¥ ë°ì´í„°ë¥¼ ì¡°íšŒí•  ë‚ ì§œë¥¼ ì„ íƒí•˜ì„¸ìš”"
    )
    
    # ìµœëŒ€ ê¸°ì‚¬ ìˆ˜ ì…ë ¥
    max_articles = st.number_input(
        "ìµœëŒ€ ê¸°ì‚¬ ìˆ˜(100ê°œ ì´í•˜)",
        min_value=10,
        max_value=1000,
        value=100,
        step=10
    )
    
    if st.button("ğŸ” ê²€ìƒ‰ ì‹œì‘", type="primary"):
        with st.spinner("ë‰´ìŠ¤ ê²€ìƒ‰ ë° ë¶„ì„ ì¤‘..."):
            # 1. ë‰´ìŠ¤ ê²€ìƒ‰ (ë‚ ì§œ ì œí•œ ì—†ìŒ)
            searcher = NaverNewsSearcher()
            articles = searcher.search_stock_news(selected_keywords, selected_date, max_articles)
            
            # í‚¤ì›Œë“œë³„ ê¸°ì‚¬ ìˆ˜ ê³„ì‚°
            keyword_article_counts = {}
            for keyword in selected_keywords:
                count = sum(1 for article in articles if keyword in article['title'] or keyword in article['description'])
                keyword_article_counts[keyword] = count
            
            # 2. ì‹œì¥ ë°ì´í„° ìˆ˜ì§‘ (ì„ íƒí•œ ë‚ ì§œ ê¸°ì¤€)
            market_data = {}
            all_stock_names = set()  # ëª¨ë“  ì¢…ëª©ëª…ì„ ì €ì¥í•  set
            
            for market in ['KOSPI', 'KOSDAQ']:
                try:
                    # app.pyì˜ collect_market_data í•¨ìˆ˜ë¥¼ importí•˜ì—¬ ì‚¬ìš©
                    from app import collect_market_data
                    df = collect_market_data(market, selected_date.strftime("%Y%m%d"))
                    market_data[market] = df
                    # ì‹œì¥ ë°ì´í„°ì—ì„œ ì¢…ëª©ëª… ì¶”ì¶œ
                    all_stock_names.update(df['ì¢…ëª©ëª…'].tolist())
                except Exception as e:
                    st.error(f"{market} ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            
            # 3. ë‰´ìŠ¤ ê¸°ì‚¬ì—ì„œ ì¢…ëª©ëª… ë§¤ì¹­
            stock_articles = {}  # ì¢…ëª©ë³„ ê¸°ì‚¬ë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬
            stock_keywords = {}  # ì¢…ëª©ë³„ ë§¤ì¹­ëœ í‚¤ì›Œë“œë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬
            matched_stocks = set()  # ë§¤ì¹­ëœ ì¢…ëª©ì„ ì¶”ì í•˜ê¸° ìœ„í•œ set
            
            for article in articles:
                # ê¸°ì‚¬ ì œëª©ê³¼ ë‚´ìš©ì—ì„œ ì¢…ëª©ëª… ì°¾ê¸°
                text = article['title'] + " " + article['description']
                
                # ê¸°ì‚¬ì— í¬í•¨ëœ í‚¤ì›Œë“œ ì°¾ê¸°
                matched_keywords = [keyword for keyword in selected_keywords if keyword in text]
                
                # ì‹œì¥ ë°ì´í„°ì˜ ëª¨ë“  ì¢…ëª©ëª…ê³¼ ë§¤ì¹­
                for stock_name in all_stock_names:
                    if stock_name in text:
                        if stock_name not in stock_articles:
                            stock_articles[stock_name] = []
                            stock_keywords[stock_name] = set()
                        stock_articles[stock_name].append(article)
                        stock_keywords[stock_name].update(matched_keywords)
                        matched_stocks.add(stock_name)
            
            # 4. ê²°ê³¼ ìƒì„±
            results = []
            for stock_name, articles in stock_articles.items():
                # í•´ë‹¹ ì¢…ëª©ì˜ ì‹œì¥ ë°ì´í„° ì°¾ê¸°
                for market, df in market_data.items():
                    stock_data = df[df['ì¢…ëª©ëª…'] == stock_name]
                    if not stock_data.empty:
                        stock = stock_data.iloc[0]
                        # ê²°ê³¼ ë°ì´í„° êµ¬ì„±
                        result = {
                            'ì¢…ëª©ëª…': stock['ì¢…ëª©ëª…'],
                            'ì‹œì¥êµ¬ë¶„': market,
                            'ì—…ì¢…': stock['ì—…ì¢…'],
                            'ì£¼ìš”ì œí’ˆ': stock['ì£¼ìš”ì œí’ˆ'],
                            'í˜„ì¬ê°€': stock['ì¢…ê°€'],
                            'ë“±ë½ë¥ ': stock['ë“±ë½ë¥ '],
                            'ê±°ë˜ëŸ‰': stock['ê±°ë˜ëŸ‰'],
                            'ì‹œê°€ì´ì•¡': stock['ì‹œê°€ì´ì•¡'],
                            'ê´€ë ¨ê¸°ì‚¬ìˆ˜': len(articles),
                            'ë§¤ì¹­í‚¤ì›Œë“œ': ', '.join(sorted(stock_keywords[stock_name]))
                        }
                        
                        # ê¸°ì‚¬ ì •ë³´ ì¶”ê°€ (ìµœëŒ€ 3ê°œ)
                        for i, article in enumerate(articles[:3], 1):
                            result.update({
                                f'ê¸°ì‚¬ì œëª©{i}': article['title'],
                                f'ê¸°ì‚¬ìš”ì•½{i}': article['description'],
                                f'ê¸°ì‚¬ë§í¬{i}': article['link']
                            })
                        
                        results.append(result)
            
            # 5. ê²€ìƒ‰ ê²°ê³¼ í†µê³„ í‘œì‹œ
            st.markdown("### ğŸ“Š ê²€ìƒ‰ ê²°ê³¼ í†µê³„")
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("#### í‚¤ì›Œë“œë³„ ê¸°ì‚¬ ìˆ˜")
                for keyword, count in keyword_article_counts.items():
                    st.write(f"- {keyword}: {count}ê°œ")
            
            with col2:
                st.markdown("#### ë§¤ì¹­ëœ ì¢…ëª© ìˆ˜")
                st.write(f"- ì´ {len(matched_stocks)}ê°œ ì¢…ëª©ì´ ë§¤ì¹­ë˜ì—ˆìŠµë‹ˆë‹¤.")
                # ì¢…ëª©ë³„ ê¸°ì‚¬ ìˆ˜ ë¶„í¬
                article_counts = [len(articles) for articles in stock_articles.values()]
                if article_counts:
                    st.write(f"- í‰ê·  {sum(article_counts)/len(article_counts):.1f}ê°œì˜ ê¸°ì‚¬ê°€ ë§¤ì¹­ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    st.write(f"- ìµœëŒ€ {max(article_counts)}ê°œì˜ ê¸°ì‚¬ê°€ ë§¤ì¹­ëœ ì¢…ëª©ì´ ìˆìŠµë‹ˆë‹¤.")
            
            # 6. ê²°ê³¼ í‘œì‹œ
            if results:
                st.markdown("### ğŸ“ˆ ë§¤ì¹­ëœ ì¢…ëª© ì •ë³´")
                df_results = pd.DataFrame(results)
                
                # ë°ì´í„° í¬ë§·íŒ…
                df_results['í˜„ì¬ê°€'] = df_results['í˜„ì¬ê°€'].apply(lambda x: f"{x:,}ì›")
                df_results['ë“±ë½ë¥ '] = df_results['ë“±ë½ë¥ '].apply(lambda x: f"{x:.2f}%")
                df_results['ê±°ë˜ëŸ‰'] = df_results['ê±°ë˜ëŸ‰'].apply(lambda x: f"{x:,}")
                df_results['ì‹œê°€ì´ì•¡'] = df_results['ì‹œê°€ì´ì•¡'].apply(lambda x: f"{x/100000000:.0f}ì–µì›")
                
                # ê²°ê³¼ í…Œì´ë¸” í‘œì‹œ
                st.dataframe(
                    df_results,
                    use_container_width=True,
                    hide_index=True
                )
                
                # CSV ë‹¤ìš´ë¡œë“œ
                csv = df_results.to_csv(index=False).encode('utf-8-sig')
                st.download_button(
                    label="ğŸ“¥ CSV ë‹¤ìš´ë¡œë“œ",
                    data=csv,
                    file_name=f"stock_news_{selected_date.strftime('%Y%m%d')}.csv",
                    mime="text/csv"
                )
            else:
                st.warning("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")