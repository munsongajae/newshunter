import warnings
import logging
import concurrent.futures
import threading
from datetime import datetime
import requests
from bs4 import BeautifulSoup
import streamlit as st
import time
import re

# Streamlit ê²½ê³  ìˆ¨ê¸°ê¸°
warnings.filterwarnings("ignore", message=".*missing ScriptRunContext.*")
logging.getLogger("streamlit").setLevel(logging.ERROR)

class NewsCollector:
    def __init__(self, headless=True):
        # ì„¤ì •ê°’ ë¡œë“œ
        try:
            self.request_delay_min = st.secrets["app_settings"].get("request_delay_min", 0.5)
            self.request_delay_max = st.secrets["app_settings"].get("request_delay_max", 1.0)
            self.max_pages_per_newspaper = st.secrets["app_settings"].get("max_pages_per_newspaper", 10)
            self.max_workers = st.secrets["app_settings"].get("max_workers", 3)
        except:
            self.request_delay_min = 0.5
            self.request_delay_max = 1.0
            self.max_pages_per_newspaper = 10
            self.max_workers = 3

        # ì„¸ì…˜ ì„¤ì • - ë” ë¹ ë¥¸ ì„¤ì •
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9',
            'Connection': 'keep-alive'
        })
        
        # ì—°ê²° í’€ ì„¤ì •ìœ¼ë¡œ ì†ë„ í–¥ìƒ
        adapter = requests.adapters.HTTPAdapter(
            pool_connections=10,
            pool_maxsize=20,
            max_retries=2
        )
        self.session.mount('http://', adapter)
        self.session.mount('https://', adapter)

    def crawl_paper_articles(self, oid, date, max_pages=None):
        """ê¸°ì¡´ app.pyì™€ í˜¸í™˜ì„±ì„ ìœ„í•œ ë©”ì„œë“œ"""
        return self.crawl_single_paper("", oid, date)

    def crawl_multiple_papers(self, paper_list, date):
        """ì—¬ëŸ¬ ì‹ ë¬¸ì‚¬ë¥¼ ë³‘ë ¬ë¡œ í¬ë¡¤ë§"""
        import pandas as pd
        
        all_articles = []
        
        # ê²°ê³¼ í…Œì´ë¸”ì„ ìœ„í•œ ë°ì´í„°
        results_data = []
        table_placeholder = st.empty()
        
        # ë³‘ë ¬ ì²˜ë¦¬
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_paper = {
                executor.submit(self.crawl_single_paper_silent, paper_name, oid, date): (paper_name, oid)
                for paper_name, oid in paper_list
            }
            
            for future in concurrent.futures.as_completed(future_to_paper):
                paper_name, oid = future_to_paper[future]
                try:
                    articles = future.result(timeout=30)
                    for article in articles:
                        article['newspaper'] = paper_name
                    all_articles.extend(articles)
                    
                    results_data.append({
                        "ì‹ ë¬¸ì‚¬": paper_name,
                        "ìƒíƒœ": "âœ… ì™„ë£Œ",
                        "ìˆ˜ì§‘ ê¸°ì‚¬": f"{len(articles)}ê°œ"
                    })
                    
                except Exception as e:
                    results_data.append({
                        "ì‹ ë¬¸ì‚¬": paper_name,
                        "ìƒíƒœ": "âŒ ì‹¤íŒ¨",
                        "ìˆ˜ì§‘ ê¸°ì‚¬": "0ê°œ"
                    })
                
                # í…Œì´ë¸” ì—…ë°ì´íŠ¸
                if results_data:
                    df = pd.DataFrame(results_data)
                    with table_placeholder:
                        st.markdown("**ğŸ“Š ìˆ˜ì§‘ í˜„í™©**")
                        st.dataframe(df, use_container_width=True, hide_index=True)
        
        return all_articles

    def crawl_single_paper(self, paper_name, oid, date):
        """ë‹¨ì¼ ì‹ ë¬¸ì‚¬ í¬ë¡¤ë§ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
        articles = []
        
        try:
            base_url = f"https://news.naver.com/main/list.naver?mode=LPOD&mid=sec&oid={oid}&listType=paper&date={date}"
            
            # ì²« í˜ì´ì§€ë§Œ ë¹ ë¥´ê²Œ í…ŒìŠ¤íŠ¸
            response = self.session.get(base_url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'lxml')
            
            # ì‹ ë¬¸ ê²Œì¬ ê¸°ì‚¬ ì˜ì—­ í™•ì¸
            main_content = soup.find('div', class_='list_body newsflash_body')
            if not main_content:
                return articles
            
            # ìµœëŒ€ 10í˜ì´ì§€ê¹Œì§€ ìˆ˜ì§‘
            for page_num in range(1, self.max_pages_per_newspaper + 1):
                try:
                    if page_num > 1:
                        page_url = f"{base_url}&page={page_num}"
                        response = self.session.get(page_url, timeout=10)
                        response.raise_for_status()
                        soup = BeautifulSoup(response.content, 'lxml')
                        main_content = soup.find('div', class_='list_body newsflash_body')
                        if not main_content:
                            break
                    
                    page_articles = self.extract_articles_fast(main_content)
                    if not page_articles:
                        break
                    
                    articles.extend(page_articles)
                    
                    # ë””ë²„ê¹… ì •ë³´ (ì‹ ë¬¸ì‚¬ëª…ì´ ìˆì„ ë•Œë§Œ)
                    if paper_name:
                        st.info(f"{paper_name} í˜ì´ì§€ {page_num}: {len(page_articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
                    
                    # ì§§ì€ ì§€ì—°
                    if page_num < self.max_pages_per_newspaper:
                        time.sleep(0.3)
                        
                except Exception as e:
                    if paper_name:
                        st.warning(f"{paper_name} í˜ì´ì§€ {page_num} ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    break
            
            # ìµœì¢… ìˆ˜ì§‘ ê²°ê³¼
            if paper_name:
                st.success(f"{paper_name}: ì´ {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
            
        except Exception as e:
            if paper_name:
                st.warning(f"{paper_name} ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        
        return articles

    def crawl_single_paper_silent(self, paper_name, oid, date):
        """ë‹¨ì¼ ì‹ ë¬¸ì‚¬ í¬ë¡¤ë§ (Streamlit í˜¸ì¶œ ì—†ëŠ” ë²„ì „)"""
        articles = []
        
        try:
            base_url = f"https://news.naver.com/main/list.naver?mode=LPOD&mid=sec&oid={oid}&listType=paper&date={date}"
            
            response = self.session.get(base_url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'lxml')
            main_content = soup.find('div', class_='list_body newsflash_body')
            if not main_content:
                return articles
            
            for page_num in range(1, self.max_pages_per_newspaper + 1):
                try:
                    if page_num > 1:
                        page_url = f"{base_url}&page={page_num}"
                        response = self.session.get(page_url, timeout=10)
                        response.raise_for_status()
                        soup = BeautifulSoup(response.content, 'lxml')
                        main_content = soup.find('div', class_='list_body newsflash_body')
                        if not main_content:
                            break
                    
                    page_articles = self.extract_articles_fast(main_content)
                    if not page_articles:
                        break
                    
                    articles.extend(page_articles)
                    
                    if page_num < self.max_pages_per_newspaper:
                        time.sleep(0.3)
                        
                except Exception as e:
                    break
            
        except Exception as e:
            pass
        
        return articles

    def extract_articles_fast(self, main_content):
        """ë¹ ë¥¸ ê¸°ì‚¬ ì¶”ì¶œ - ëª¨ë“  êµ¬ì¡° í†µí•© ì²˜ë¦¬"""
        articles = []
        
        try:
            # ëª¨ë“  ê¸°ì‚¬ ë§í¬ë¥¼ í•œ ë²ˆì— ì°¾ê¸°
            article_links = main_content.find_all('a', href=lambda x: x and 'mnews/article' in x)
            
            for link in article_links:
                try:
                    href = link.get('href')
                    title = link.get_text().strip()
                    
                    if not href or not title or len(title) < 3:
                        continue
                    
                    # ì ˆëŒ€ URLë¡œ ë³€í™˜
                    if href.startswith('/'):
                        href = f"https://news.naver.com{href}"
                    
                    # ë©´ ì •ë³´ ì¶”ì¶œ - ì—¬ëŸ¬ ìœ„ì¹˜ì—ì„œ ì°¾ê¸°
                    page_info = self.extract_page_info_comprehensive(link)
                    
                    # ì¤‘ë³µ ì²´í¬
                    if not any(article['url'] == href for article in articles):
                        articles.append({
                            'title': title,
                            'url': href,
                            'page': page_info,
                            'collected_at': datetime.now().isoformat()
                        })
                        
                except Exception:
                    continue
            
        except Exception as e:
            pass
        
        return articles

    def extract_page_info_comprehensive(self, link_element):
        """ë§í¬ ì£¼ë³€ì—ì„œ ë©´ ì •ë³´ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ì¶”ì¶œ"""
        try:
            # 1. ê°™ì€ dt ì•ˆì—ì„œ ì°¾ê¸° (ì¼ë°˜ ê¸°ì‚¬)
            dt_parent = link_element.find_parent('dt')
            if dt_parent:
                newspaper_info = dt_parent.find('span', class_='newspaper_info')
                if newspaper_info:
                    info_text = newspaper_info.get_text()
                    page_match = re.search(r'([A-Z]?\d+ë©´)', info_text)
                    if page_match:
                        return page_match.group(1)
            
            # 2. dl > dd êµ¬ì¡°ì—ì„œ ì°¾ê¸° (í†±ê¸°ì‚¬)
            dl_parent = link_element.find_parent('dl')
            if dl_parent:
                dd = dl_parent.find('dd')
                if dd:
                    newspaper_info = dd.find('span', class_='newspaper_info')
                    if newspaper_info:
                        info_text = newspaper_info.get_text()
                        page_match = re.search(r'([A-Z]?\d+ë©´)', info_text)
                        if page_match:
                            return page_match.group(1)
            
            # 3. ìƒìœ„ ìš”ì†Œë“¤ì—ì„œ ì°¾ê¸° (ê¸°íƒ€ êµ¬ì¡°)
            current = link_element.parent
            for _ in range(3):  # ìµœëŒ€ 3ë‹¨ê³„ ë¶€ëª¨ê¹Œì§€
                if current:
                    newspaper_info = current.find('span', class_='newspaper_info')
                    if newspaper_info:
                        info_text = newspaper_info.get_text()
                        page_match = re.search(r'([A-Z]?\d+ë©´)', info_text)
                        if page_match:
                            return page_match.group(1)
                    current = current.parent
                else:
                    break
            
            return ""
        except:
            return ""

    def close(self):
        """ì„¸ì…˜ ì¢…ë£Œ"""
        try:
            self.session.close()
        except:
            pass

    def get_newspaper_categories(self):
        """ì‹ ë¬¸ì‚¬ ì¹´í…Œê³ ë¦¬ ì •ë³´ ë°˜í™˜"""
        return {
            "ê²½ì œì‹ ë¬¸": {
                "ë§¤ì¼ê²½ì œ": "009",
                "ë¨¸ë‹ˆíˆ¬ë°ì´": "008", 
                "ì„œìš¸ê²½ì œ": "011",
                "ì´ë°ì¼ë¦¬": "018",
                "íŒŒì´ë‚¸ì…œë‰´ìŠ¤": "014",
                "í•œêµ­ê²½ì œ": "015"
            },
            "ì¢…í•©ì¼ê°„ì§€": {
                "ê²½í–¥ì‹ ë¬¸": "032",
                "êµ­ë¯¼ì¼ë³´": "005",
                "ë™ì•„ì¼ë³´": "020",
                "ì„œìš¸ì‹ ë¬¸": "081",
                "ì„¸ê³„ì¼ë³´": "022",
                "ì¡°ì„ ì¼ë³´": "023",
                "ì¤‘ì•™ì¼ë³´": "025",
                "í•œê²¨ë ˆ": "028",
                "í•œêµ­ì¼ë³´": "469",
                "ë””ì§€í„¸íƒ€ì„ìŠ¤": "029",
                "ì „ìì‹ ë¬¸": "030"
            },
            "ì„ê°„ì‹ ë¬¸": {
                "ë¬¸í™”ì¼ë³´": "021",
                "í—¤ëŸ´ë“œê²½ì œ": "016", 
                "ì•„ì‹œì•„ê²½ì œ": "277"
            }
        }
